{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook\n",
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import glob\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, inputs, labels):\n",
    "        assert len(labels) == len(inputs)\n",
    "        assert isinstance(inputs, torch.Tensor)\n",
    "        assert isinstance(labels, torch.Tensor)\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, input_path):\n",
    "        join = os.path.join        \n",
    "        self.training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "        self.training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "        self.test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "        self.test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')        \n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        #for i in range(size):\n",
    "        #    images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images.append(img)            \n",
    "        \n",
    "        inputs = torch.stack([torch.tensor(image) for image in images])\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        assert list(inputs.shape)[1:] == [1, 28, 28]\n",
    "\n",
    "        inputs = (inputs * (2.0/255.0)) - 1.0\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "        assert labels.shape[0] == inputs.shape[0]\n",
    "        return Data(inputs, labels)\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        train_data = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        test_data = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return train_data, test_data\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(inputs, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(inputs)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for input, title_text in zip(inputs, title_texts):\n",
    "        image = (input[0] + 1.0) * 255.0        \n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/home/dking/data/mnist/\"\n",
    "loader = MnistDataloader(os.path.join(project_dir, 'raw'))\n",
    "train_data, test_data = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(train_data.inputs[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(train_data.labels[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MnistNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 28x28 input (example assumes 32x32)\n",
    "        # 28 - 4 = 24\n",
    "        # 24/2 = 12\n",
    "        # 12 - 4 = 8\n",
    "        # 8/2 = 4\n",
    "        # 16 channel * 4x4\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self, data, predictions, failure_idxs):\n",
    "        self.data = data\n",
    "        self.predictions = predictions\n",
    "        self.failure_idxs = failure_idxs\n",
    "        assert predictions.shape == self.data.labels.shape\n",
    "\n",
    "    def failure_ratio(self):\n",
    "        return len(self.failure_idxs) / len(self.predictions)\n",
    "\n",
    "def compute_results(net, data):\n",
    "    batch_size = 16\n",
    "    failure_idxs = []\n",
    "    predictions = torch.zeros(data.labels.shape, dtype=torch.int)\n",
    "    with torch.no_grad():\n",
    "        # TODO is there a good way to batch this?\n",
    "        for idx in range(0, len(data.inputs), batch_size):\n",
    "            label = data.labels[idx:idx+batch_size]\n",
    "            input = data.inputs[idx:idx+batch_size]\n",
    "            output = net(input)\n",
    "            _, predict = torch.max(output, 1)\n",
    "            predictions[idx:idx+batch_size] = predict\n",
    "            errs = predict != label\n",
    "            if torch.any(errs):\n",
    "                for i, (p,l) in enumerate(zip(predict, label)):\n",
    "                    if p != l:\n",
    "                        failure_idxs.append(idx+i)\n",
    "    return Results(data, predictions, failure_idxs)\n",
    "\n",
    "def print_result_summary(results, name):\n",
    "    print(f\"{name} : error {len(results.failure_idxs)/len(results.data.labels)*100.0:0.1f}%\")\n",
    "\n",
    "def print_result_breakdown(results, name):\n",
    "    print_result_summary(results, name)\n",
    "    err_counts = defaultdict(int)\n",
    "    for idx in results.failure_idxs:\n",
    "        err_counts[(results.predictions[idx].item(), results.data.labels[idx].item())] += 1\n",
    "    if len(err_counts) > 0:\n",
    "        print(\"  LABEL  PREDICT  :  COUNT\")\n",
    "        err_counts = sorted([(c, p, l) for (p, l), c in err_counts.items()], reverse=True)\n",
    "        for c, p, l in err_counts[:10]:\n",
    "            print(f\"  {l:3d}      {p:3d}    :  {c}\")\n",
    "\n",
    "def show_failures(results, count=10):\n",
    "    idxs = random.sample(results.failure_idxs, count)\n",
    "    inputs = results.data.inputs[idxs]\n",
    "    titles = [f'{i}, expect {results.data.labels[i].item()} got {results.predictions[i].item()}' for i in idxs]    \n",
    "    show_images(inputs, titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "def train(net_name, opt_name, batch_size=16, epoch_count=30):\n",
    "    torch.manual_seed(0)\n",
    "    if net_name == \"net1\":\n",
    "        net = MnistNet1()\n",
    "    else:\n",
    "        raise RuntimeError(\"invalid net name\")\n",
    "\n",
    "    print(net)\n",
    "    print(\"Total Param Count\", sum([np.prod(p.size()) for p in net.parameters()]))\n",
    "\n",
    "    if opt_name == 'sgd1':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.0)\n",
    "    elif opt_name == 'sgd2':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    elif opt_name == 'sgdN':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "    elif opt_name == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters())\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=0.001)\n",
    "    else:\n",
    "        raise RuntimeError(\"invalid opt name\")\n",
    "\n",
    "    print(optimizer)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(train_data.dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    train_failures = []\n",
    "    test_failures = []\n",
    "    losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch_count):\n",
    "        print(f\"EPOCH {epoch:02d}\")\n",
    "        running_loss = []\n",
    "\n",
    "        for input, target in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        avg_loss = float(np.array(running_loss).mean())\n",
    "        print(f\"  LOSS {avg_loss:0.3f}\")\n",
    "        test_results = compute_results(net, test_data)\n",
    "        train_results = compute_results(net, train_data)\n",
    "        print_result_summary(train_results, \"  TRAIN\")\n",
    "        print_result_summary(test_results, \"  TEST\")\n",
    "        train_failures.append(train_results.failure_ratio())\n",
    "        test_failures.append(test_results.failure_ratio())\n",
    "        losses.append(avg_loss)\n",
    "    training_duration = time.time() - start\n",
    "\n",
    "    fn = os.path.join(project_dir, 'results', f'training_{net_name}_{opt_name}.yaml')\n",
    "    print(\"Saving results to\", fn)\n",
    "    with open(fn, 'w') as fd:\n",
    "        yaml.safe_dump({'net_name': net_name, \n",
    "                        'opt_name':opt_name, \n",
    "                        'losses': losses, \n",
    "                        'train_failures': train_failures,\n",
    "                        'test_failures': test_failures,\n",
    "                        'training_duration': training_duration,\n",
    "                        }, \n",
    "                        stream=fd)\n",
    "\n",
    "    train_results = compute_results(net, train_data)\n",
    "    test_results = compute_results(net, test_data)\n",
    "    return (train_results, test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for opt_name in ('rmsprop', ):\n",
    "for opt_name in ('sgd1', 'sgd2', 'sgdN', 'adam'):\n",
    "    train('net1', opt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "fns = sorted(glob.glob(os.path.join(project_dir, 'results', 'training*.yaml')))\n",
    "print(fns)\n",
    "\n",
    "training_durations = {}\n",
    "\n",
    "plt.figure(\"training\", figsize=(20,15))\n",
    "for fn in fns:\n",
    "    with open(fn, 'r') as fd:\n",
    "        y = yaml.safe_load(fd)\n",
    "    label = f\"{y['net_name']},{y['opt_name']}\"\n",
    "    training_durations[label] = y['training_duration']\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(np.log(y['losses']), '.-', label=label)\n",
    "    plt.ylabel('log loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.ylabel('log train failure count')\n",
    "    plt.plot(np.log(y['train_failures']), '.-', label=label)\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.ylabel('log test failure count')\n",
    "    plt.plot(np.log(y['test_failures']), '.-', label=label)\n",
    "    plt.legend(loc='best')\n",
    "plt.show(block=False)\n",
    "\n",
    "print(\"OPTIMIZER  : DURATION\")\n",
    "for label, duration in training_durations.items():\n",
    "    print(f\"{label:<10} : {duration:.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
